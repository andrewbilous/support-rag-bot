# ğŸ§  RAG-Powered PDF QA Bot

This repository contains a minimal Retrieval-Augmented Generation (RAG) pipeline for answering questions based on the content of a PDF file using **LangChain**, **OpenAI GPT**, **HuggingFace Embeddings**, and **FAISS**.

---

## âœ¨ Features

- âœ… Load and parse PDF documents
- âœ… Chunk text intelligently using LangChain
- âœ… Embed text using HuggingFace models (`thenlper/gte-small`)
- âœ… Store embeddings in a FAISS vector index
- âœ… Use GPT-3.5-Turbo to answer user queries with retrieved context
- âœ… `.env`-based secret management

---

## ğŸ“¦ Tech Stack

- [LangChain](https://python.langchain.com/)
- [OpenAI GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [FAISS](https://github.com/facebookresearch/faiss)
- [Python dotenv](https://pypi.org/project/python-dotenv/)
- [PyPDF](https://pypi.org/project/pypdf/)

---

## ğŸ“ File Structure

```
support-rag-bot/
â”‚
â”œâ”€â”€ main.py                  # Main RAG pipeline
â”œâ”€â”€ support-policy.pdf       # Source document
â”œâ”€â”€ .env                     # Environment variables (API keys)
â”œâ”€â”€ README.md                # Project documentation
â””â”€â”€ requirements.txt         # Python dependencies
```

---

## âš™ï¸ Setup

### 1. Clone the repo

```bash
git clone git@github.com:andrewbilous/support-rag-bot.git
cd support-rag-bot
```

### 2. Create and activate virtual environment

```bash
python3 -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Create `.env` file

Create a `.env` file in the project root with the following content:

```env
OPENAI_API_KEY=your-openai-api-key
HUGGINGFACEHUB_API_TOKEN=your-huggingface-token
```

---

## â–¶ï¸ Usage

Make sure you have a PDF file named `support-policy.pdf` in the project root.

Then run the main script:

```bash
python main.py
```

To query the document:

```python
response = qa_chain.run("What is the refund policy?")
print(response)
```

You can modify the question inside `main.py` or turn this into an interactive CLI later.

---

## ğŸ“Œ Notes

- `ChatOpenAI` is used as the LLM to generate final answers.
- `FAISS` provides fast in-memory similarity search over embedded chunks.
- Embedding is done with HuggingFaceâ€™s `thenlper/gte-small` model.
- The PDF is parsed and split into overlapping chunks for better context retrieval.

---

## ğŸ“„ requirements.txt (example)

```txt
langchain
openai
python-dotenv
transformers
torch
faiss-cpu
pypdf
```

---

## ğŸš§ Future Improvements

- [ ] Add Gradio or Streamlit frontend
- [ ] Add support for multiple PDFs
- [ ] Cache FAISS index
- [ ] Switch to local LLM for offline use (e.g. LLaMA or Mistral)

---

## ğŸ‘¨â€ğŸ’» Author

**Andrii Bilous**  
[GitHub Profile](https://github.com/andrewbilous)

---

## ğŸªª License

This project is licensed under the [MIT License](LICENSE).
